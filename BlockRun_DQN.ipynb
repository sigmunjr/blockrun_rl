{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa35918",
   "metadata": {},
   "source": [
    "# BlockRun DQN Tutorial\n",
    "This notebook demonstrates how to train a Deep Q-Network (DQN) agent to play BlockRun.\n",
    "It also shows how to visualise training progress and play the game manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e5578",
   "metadata": {},
   "source": [
    "## How DQN works\n",
    "1. **Environment** provides observations of the game state.\n",
    "2. **Q-network** estimates action values for each possible move.\n",
    "3. **Target network** stabilises training by updating more slowly.\n",
    "4. **Experience** is gathered by acting \"epsilon-greedily\" (mostly best action, sometimes random).\n",
    "5. **Loss** is the difference between predicted Q-values and target values using the Bellman equation.\n",
    "6. The network is updated with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from blockrun import BlockRun, map_view_to_image, play_game\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ccf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example view from the environment\n",
    "example_env = BlockRun(50, device=device)\n",
    "view = example_env.render()\n",
    "plt.imshow(map_view_to_image(view))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e15a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, (5, 3), stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (5, 3), stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, (5, 3), stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, (13, 5), padding='valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 4, (1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).mean(dim=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(num_iterations=2000, epsilon=0.1, gamma=0.99):\n",
    "    q_network = QNetwork().to(device)\n",
    "    target_network = QNetwork().to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = torch.optim.Adam(q_network.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    env = BlockRun(2_000_000, device=device)\n",
    "    rewards = []\n",
    "    for i in range(num_iterations):\n",
    "        view = env.render().float().unsqueeze(0).to(device)\n",
    "        q_values = q_network(view)\n",
    "        if torch.rand(1) < epsilon:\n",
    "            action = torch.randint(0, 4, (1,))\n",
    "        else:\n",
    "            action = torch.argmax(q_values)\n",
    "        env.step(action.item())\n",
    "        next_view = env.render().float().unsqueeze(0).to(device)\n",
    "        next_q = target_network(next_view)\n",
    "        target = q_values.clone()\n",
    "        target[0, action] = env.current_reward + gamma * torch.max(next_q)\n",
    "        loss = loss_fn(q_values, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        rewards.append(env.current_reward)\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "    return q_network, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4958a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net, rewards = train_dqn(num_iterations=5000)\n",
    "avg_rewards = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "plt.plot(avg_rewards)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('100-step average reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(q_network, steps=200):\n",
    "    env = BlockRun(1000, device=device)\n",
    "    total_reward = 0\n",
    "    for _ in range(steps):\n",
    "        view = env.render().float().unsqueeze(0).to(device)\n",
    "        action = torch.argmax(q_network(view)).item()\n",
    "        env.step(action)\n",
    "        total_reward += env.current_reward\n",
    "    print('Total reward over', steps, 'steps:', total_reward)\n",
    "run_agent(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96478d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to play the game manually using WASD keys.\n",
    "play_game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
